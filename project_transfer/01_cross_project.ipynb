{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26847da5",
   "metadata": {},
   "source": [
    "# Testing cross project artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U xgboost\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c372dd6b",
   "metadata": {},
   "source": [
    "## 1. artifact generating function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlrun: start-code\n",
    "\n",
    "import mlrun\n",
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from mlrun.artifacts.base import DirArtifact\n",
    "from mlrun import MLClientCtx\n",
    "from sklearn.datasets import load_iris\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_dataitem(context: MLClientCtx,\n",
    "                             key: str):\n",
    "    \n",
    "    for artifact in context.artifacts:\n",
    "        if artifact['kind'] == 'model' and artifact['metadata'].get('key',None) == key:\n",
    "            return mlrun.get_dataitem(artifact['spec']['target_path'] + artifact['spec']['model_file'])\n",
    "        elif artifact['kind'] == 'dataset' and artifact['metadata'].get('key',None) == key:\n",
    "            return mlrun.get_dataitem(artifact['spec']['target_path'])\n",
    "        elif artifact['metadata'].get('key',None) == key:\n",
    "            return mlrun.get_dataitem(artifact['spec']['target_path'])\n",
    "    context.logger.info('Artifact not found')\n",
    "    \n",
    "def log_transactions(context: MLClientCtx,\n",
    "                    ):\n",
    "        \n",
    "    # uploading new artifact \n",
    "    df_encode = pd.DataFrame(load_iris()['data']).to_json().encode()\n",
    "    context.log_artifact('encoded_iris-'+context.artifact_path[:2], body=df_encode, local_path='encoded_iris-'+context.artifact_path[:2]+'.csv')\n",
    "    # reading artifact\n",
    "    trans_df = pd.DataFrame(json.loads(get_dataitem(context, 'encoded_iris-'+context.artifact_path[:2]).get()))\n",
    "    context.logger.info(f'dataframe shape : {trans_df.shape}')\n",
    "    \n",
    "    # training the model (for serving purposes )\n",
    "    bst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
    "    X,y = load_iris(return_X_y=True)\n",
    "    X_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n",
    "    bst.fit(X_train, y_train)\n",
    "    # logging a model\n",
    "    context.log_model('bst_model', body=pickle.dumps(bst), model_file='bst.pkl')\n",
    "    # getting the model remote\n",
    "    model = pickle.loads(get_dataitem(context, 'bst_model').get())\n",
    "    context.logger.info(f'logged model : {model.__class__}')\n",
    "    \n",
    "    # Logging directory\n",
    "    context.log_artifact(DirArtifact(key='my_project', target_path=context.artifact_path))\n",
    "    \n",
    "    # Logging dataset \n",
    "    context.log_dataset(key = 'iris_dataset-'+context.artifact_path[:2],\n",
    "                        df = pd.DataFrame(json.loads(get_dataitem(context, 'encoded_iris-'+context.artifact_path[:2]).get())),\n",
    "                        local_path='iris_dataset-'+context.artifact_path[:2]+'.csv')\n",
    "    # Getting dataset\n",
    "    context.logger.info(f'logged dataset {get_dataitem(context, \"iris_dataset-\" + context.artifact_path[:2]).as_df().shape}')\n",
    "    \n",
    "    return\n",
    "            \n",
    "#mlrun: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfdfede",
   "metadata": {},
   "source": [
    "## 2. Creating projects, setting & running artifact generating function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ce2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "import os\n",
    "\n",
    "# Initialize the MLRun project object\n",
    "project1 = mlrun.get_or_create_project('cross-project1',user_project=True,context=os.path.join(os.getcwd(), 'test-notebooks1'))\n",
    "\n",
    "func = project1.set_function(name='log_transactions', kind='job', image='mlrun/mlrun', handler='log_transactions', requirements=[\"xgboost\"])\n",
    "func.deploy()\n",
    "project1.get_function('log_transactions').run(local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a987b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLRun project object\n",
    "project2 = mlrun.get_or_create_project('cross-project2',user_project=True,context=os.path.join(os.getcwd(), 'test-notebooks2'))\n",
    "\n",
    "func = project2.set_function(name='log_transactions', kind='job', image='mlrun/mlrun', handler='log_transactions', requirements=[\"xgboost\"])\n",
    "func.deploy()\n",
    "project2.get_function('log_transactions').run(local=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e43fd3",
   "metadata": {},
   "source": [
    "## 3. Importing/Exporting artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6aade",
   "metadata": {},
   "source": [
    "### 3.1 base artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb58f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting project2 artifact\n",
    "project2.get_artifact('log-transactions-log-transactions_encoded_iris-v3').export('proj2_encoded_iris.yaml')\n",
    "\n",
    "# Importing the artifact from project1\n",
    "project1.import_artifact('../proj2_encoded_iris.yaml',\n",
    "                         new_key = 'imported_proj1_encoded_iris')\n",
    "\n",
    "# Testing the imported artifact\n",
    "pd.DataFrame(json.loads(project1.get_artifact('imported_proj1_encoded_iris').to_dataitem().get())).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac096eb",
   "metadata": {},
   "source": [
    "### 3.2 dataset artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting project2 model artifact\n",
    "project2.get_artifact('log-transactions-log-transactions_iris_dataset-v3').export('proj2_dataset.yaml')\n",
    "\n",
    "# importing model artifact from project2\n",
    "project1.import_artifact('../proj2_dataset.yaml',\n",
    "                         new_key = 'imported_proj1_dataset')\n",
    "\n",
    "# Testing the imported dataset artifact\n",
    "project1.get_artifact('imported_proj1_dataset').to_dataitem().as_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9944449",
   "metadata": {},
   "source": [
    "### 3.3 model artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba42557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting project1 model artifact\n",
    "project1.get_artifact('log-transactions-log-transactions_bst_model').export('proj1_model.yaml')\n",
    "\n",
    "# importing model artifact from project2\n",
    "project2.import_artifact('../proj1_model.yaml',\n",
    "                         new_key = 'imported_proj2_model')\n",
    "\n",
    "\n",
    "# Testing the imported model artifact\n",
    "project2_model = pickle.loads(project2.get_artifact('imported_proj2_model')._get_file_body())\n",
    "project2_model.predict(pd.DataFrame(json.loads((project1.get_artifact('log-transactions-log-transactions_encoded_iris-v3').to_dataitem().get())))[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331dad14",
   "metadata": {},
   "source": [
    "### 3.4 dir artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    # Exporting project2 dir artifact\n",
    "    project1.get_artifact('log-transactions-log-transactions_my_project').export('proj1_dir.yaml')\n",
    "\n",
    "    # importing dir artifact from project2\n",
    "    project2.import_artifact('../proj1_dir.yaml',\n",
    "                             new_key = 'imported_proj2_dir')\n",
    "\n",
    "    # Testing the imported dir artifact\n",
    "    print(project2.get_artifact('imported_proj2_dir').to_dataitem().listdir())\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66afce",
   "metadata": {},
   "source": [
    "## 4. Importing/Exporting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937171d",
   "metadata": {},
   "source": [
    "### 4.1 Serving function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2580c79d",
   "metadata": {},
   "source": [
    "#### 4.1.1 Creating serving function, adding model, predicting and deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd48bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting project1 serving function\n",
    "func = project1.set_function(name='proj1-serving',func = mlrun.new_function(name='proj1-serving', kind='serving',image='mlrun/mlrun', command = [], requirements=[\"xgboost\"]))\n",
    "# adding project2 model to project1 serving function\n",
    "project1.get_function('proj1-serving').add_model(key = 'my_model',\n",
    "                                                 class_name = \"mlrun.frameworks.xgboost.XGBoostModelServer\",\n",
    "                                                 model_path = project2.get_artifact('log-transactions-log-transactions_bst_model').target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a98641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the fused model serving function\n",
    "server = project1.get_function('proj1-serving').to_mock_server()\n",
    "\n",
    "server.test(body={'inputs': \n",
    "                  pd.DataFrame(json.loads(project2.get_artifact('log-transactions-log-transactions_encoded_iris-v3').to_dataitem().get())).values.tolist()[-5:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project1.deploy_function('proj1-serving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a838eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)\n",
    "\n",
    "project1.get_function('proj1-serving').invoke(\n",
    "    path='/v2/models/my_model/infer', \n",
    "    body={'inputs': pd.DataFrame(json.loads(project2.get_artifact('log-transactions-log-transactions_encoded_iris-v3').to_dataitem().get())).values.tolist()[-5:]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e943f7c",
   "metadata": {},
   "source": [
    "#### 4.1.2 Importing & Exporting already deployed serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6366eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting project1 deployed serving function\n",
    "project1.get_function('proj1-serving').export('proj1-serving.yaml')\n",
    "\n",
    "# importing project1 serving function from project2\n",
    "project2.set_function(name='proj2-imported-serving', func='../proj1-serving.yaml')\n",
    "print(project2.get_function('proj2-imported-serving').is_deployed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Testing the imported already deployed function\n",
    "    server = project2.get_function('proj2-imported-serving').to_mock_server()\n",
    "\n",
    "    server.test(body={'inputs': \n",
    "                      pd.DataFrame(json.loads(project2.get_artifact('log-transactions-log-transactions_encoded_iris-v3').to_dataitem().get())).values.tolist()[-5:]})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da62fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redeploying the already deployed function\n",
    "project2.deploy_function('proj2-imported-serving')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958bcc21",
   "metadata": {},
   "source": [
    "##### Testing the redeployed imported-deployed-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1affae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5) \n",
    "project2.get_function('proj2-imported-serving').invoke(\n",
    "    path='/v2/models/my_model/infer', \n",
    "    body={'inputs': pd.DataFrame(json.loads(project2.get_artifact('log-transactions-log-transactions_encoded_iris-v3').to_dataitem().get())).values.tolist()[-5:]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6ff021",
   "metadata": {},
   "source": [
    "#### 4.1.3 Importing & Exporting undeployed serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting project2 serving function\n",
    "func = project2.set_function(mlrun.new_function(name='proj2-serving', kind='serving',image='mlrun/mlrun', command = [], requirements=[\"xgboost\"]))\n",
    "# adding project1 model to project2 serving function\n",
    "project2.get_function('proj2-serving').add_model(key = 'my_model',\n",
    "                                                 class_name = \"mlrun.frameworks.xgboost.XGBoostModelServer\",\n",
    "                                                 model_path = project1.get_artifact('log-transactions-log-transactions_bst_model').target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting project2 not deployed serving function\n",
    "project2.get_function('proj2-serving').export('proj2-serving.yaml')\n",
    "\n",
    "# Importing project2 serving function from project1\n",
    "project1.set_function(name='proj1-imported-serving', func='../proj2-serving.yaml')\n",
    "print(project1.get_function('proj1-imported-serving').is_deployed())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e736a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the imported undeployed function\n",
    "server = project1.get_function('proj1-imported-serving').to_mock_server()\n",
    "\n",
    "server.test(body={'inputs': \n",
    "                  pd.DataFrame(json.loads(project2.get_artifact('log-transactions-log-transactions_encoded_iris-v3').to_dataitem().get())).values.tolist()[-5:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb7255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying the undeployed imported function\n",
    "project1.deploy_function('proj1-imported-serving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67962a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "\n",
    "# Testing the imported then deployed function\n",
    "project1.get_function('proj1-imported-serving').invoke(\n",
    "    path='/v2/models/my_model/infer', \n",
    "    body={'inputs': pd.DataFrame(json.loads(project2.get_artifact('log-transactions-log-transactions_encoded_iris-v3').to_dataitem().get())).values.tolist()[-5:]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7123e6d",
   "metadata": {},
   "source": [
    "### 4.2 mlrun function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24067b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting project2 log_transaction mlrun function\n",
    "project2.get_function('log-transactions').export('proj2-mlrun-func.yaml')\n",
    "\n",
    "# Importing project2 log_transaction mlrun function from project1\n",
    "project1.set_function(name='proj1-imported-mlrun-func', func='../proj2-mlrun-func.yaml')\n",
    "project1.run_function('proj1-imported-mlrun-func')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87e6a2",
   "metadata": {},
   "source": [
    "### 4.3 nuclio function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422a9db",
   "metadata": {},
   "source": [
    "#### 4.3.1 Importing & Exporting deployed remote function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile script.py\n",
    "import mlrun\n",
    "import time\n",
    "\n",
    "def handler(context: mlrun.MLClientCtx, event):\n",
    "    context.logger.info('Going to sleep zZz...')\n",
    "    time.sleep(5)\n",
    "    return 'Waking up !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198dea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting remote function\n",
    "project1.set_function(func = mlrun.code_to_function(name='proj1-remote-func', kind='remote',image='mlrun/mlrun', filename='script.py', handler='handler', requirements=[\"xgboost\"]))\n",
    "\n",
    "# Deploying\n",
    "project1.deploy_function('proj1-remote-func')\n",
    "# Exporting project1 undeployed remote function\n",
    "project1.get_function('proj1-remote-func').export('proj1-remote-func.yaml')\n",
    "\n",
    "# Importing project2 remote function from project1\n",
    "project2.set_function(name='proj2-imported-remote-func', func='../proj1-remote-func.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667644c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "project2.deploy_function('proj2-imported-remote-func')\n",
    "\n",
    "time.sleep(5)\n",
    "project2.get_function('proj2-imported-remote-func').invoke('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c317fcb7",
   "metadata": {},
   "source": [
    "## 5. Importing/Exporting project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1d487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLRun project object\n",
    "new_project = mlrun.get_or_create_project('testing-exported-proj', context=os.path.join(os.getcwd(), 'new-project'))\n",
    "\n",
    "# Required credentials :\n",
    "# AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, GOOGLE_APPLICATION_CREDENTIALS, S3_BUCKET\n",
    "AWS_ACCESS_KEY_ID = os.environ.get('AWS_ACCESS_KEY_ID', None)\n",
    "AWS_SECRET_ACCESS_KEY = os.environ.get('AWS_SECRET_ACCESS_KEY', None)\n",
    "GOOGLE_APPLICATION_CREDENTIALS = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', None)\n",
    "assert AWS_ACCESS_KEY_ID != None and AWS_SECRET_ACCESS_KEY != None and GOOGLE_APPLICATION_CREDENTIALS != None\n",
    "\n",
    "secrets = {'AWS_ACCESS_KEY_ID': AWS_ACCESS_KEY_ID,\n",
    "           'AWS_SECRET_ACCESS_KEY':AWS_SECRET_ACCESS_KEY,\n",
    "           'GOOGLE_APPLICATION_CREDENTIALS': GOOGLE_APPLICATION_CREDENTIALS}\n",
    "\n",
    "new_project.set_secrets(secrets=secrets, provider='kubernetes')\n",
    "\n",
    "S3_BUCKET = os.environ.get('S3_BUCKET', 'testbucket-igz-temp')\n",
    "GS_BUCKET = os.environ.get('GS_BUCKET', 'testbucket-igz')\n",
    "\n",
    "new_project.artifact_path = os.path.join('s3://', S3_BUCKET + '/cross_project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2d09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting artifacts\n",
    "new_project.set_artifact('encoded_iris', artifact=os.path.join(os.getcwd(),'proj2_encoded_iris.yaml'))\n",
    "new_project.set_artifact('model', artifact=os.path.join(os.getcwd(),'proj1_model.yaml'))\n",
    "new_project.set_artifact('dataset', artifact=os.path.join(os.getcwd(),'proj2_dataset.yaml'))\n",
    "new_project.set_artifact('dir', artifact=os.path.join(os.getcwd(),'proj1_dir.yaml'))\n",
    "\n",
    "# Setting functions\n",
    "new_project.set_function(name='serving', func='../proj1-serving.yaml')\n",
    "new_project.set_function(name='mlrun-func', func='../proj2-mlrun-func.yaml')\n",
    "new_project.set_function(name='remote-func', func='../proj1-remote-func.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the project\n",
    "\n",
    "# S3 artifact path (e.g. s3://my-bucket/new_project.zip)\n",
    "new_project.export(filepath=os.path.join(new_project.artifact_path, 'new_project.zip'))\n",
    "\n",
    "# GCS artifact path (e.g. gs://my-bucket/new_project.zip)\n",
    "new_project.export(filepath=os.path.join('gs://' + GS_BUCKET, 'new_project.zip'))\n",
    "\n",
    "# V3IO local path\n",
    "new_project.export(filepath='/v3io/bigdata/new_project.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ea657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the projects\n",
    "gs_project = mlrun.load_project(name='testing-gs-project',\n",
    "                                url=os.path.join(new_project.artifact_path, 'new_project.zip'),\n",
    "                                context = os.path.join(os.getcwd(), 'gs-project'))\n",
    "\n",
    "s3_project = mlrun.load_project(name='testing-s3-project',\n",
    "                                url=os.path.join('s3' + new_project.artifact_path[2:], 'new_project.zip'),\n",
    "                                context = os.path.join(os.getcwd(), 's3-project'))\n",
    "\n",
    "v3io_project = mlrun.load_project(name='testing-v3io-project',\n",
    "                                  url='/v3io/bigdata/new_project.zip',\n",
    "                                  context=os.path.join(os.getcwd(), 'v3io-project'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026eb87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the imported artifacts\n",
    "# Artifact\n",
    "gs_encoded_dataset = pd.DataFrame(json.loads(gs_project.get_artifact('encoded_iris').to_dataitem().get()))\n",
    "print('gs project artifact\\n', gs_encoded_dataset.head(), '\\n\\n')\n",
    "\n",
    "# Dataset\n",
    "s3_dataset = s3_project.get_artifact('dataset').to_dataitem().as_df()\n",
    "print('s3 project artifact\\n',s3_dataset.head())\n",
    "\n",
    "# Model\n",
    "v3io_model = pickle.loads(v3io_project.get_artifact('model')._get_file_body())\n",
    "v3io_model.predict(gs_encoded_dataset[:5])\n",
    "\n",
    "# Directory\n",
    "gs_dir = gs_project.get_artifact('dir').to_dataitem().listdir()\n",
    "\n",
    "# Testing imported functions\n",
    "# Serving function\n",
    "gs_project.get_function('serving').deploy()\n",
    "time.sleep(5)\n",
    "gs_project.get_function('serving').invoke(\n",
    "    path='/v2/models/my_model/infer', \n",
    "    body={'inputs': gs_encoded_dataset.values[-5:].tolist()}\n",
    ")\n",
    "\n",
    "# mlrun function\n",
    "s3_project.run_function('mlrun-func',local=True)\n",
    "\n",
    "## nuclio function\n",
    "v3io_project.deploy_function('remote-func')\n",
    "\n",
    "time.sleep(5)\n",
    "v3io_project.get_function('remote-func').invoke('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c05cd7d",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a821ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = [project1,project2,gs_project,s3_project,v3io_project, new_project]\n",
    "for project in projects:\n",
    "    mlrun.get_run_db().delete_project(name=project.name, deletion_strategy='cascade')\n",
    "\n",
    "import shutil\n",
    "for f in os.listdir():\n",
    "    if (not f.endswith('ipynb')) and f != '.test':\n",
    "        if os.path.isfile(f):\n",
    "            os.remove(f)\n",
    "        elif os.path.isdir(f):\n",
    "            shutil.rmtree(f)\n",
    "        else:\n",
    "            raise \"A file that is not a notebook wasn't deleted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1430ca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(urlparse(project.artifact_path).netloc)\n",
    "bucket.objects.filter(Prefix=urlparse(project.artifact_path).path[1:]).delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
