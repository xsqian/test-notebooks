{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78ad2e5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. Create a kafka service named my-kafka on default-tenant namespace\n",
    "2. creating a kafka input, output,error topics and broker address equal to the keys below, each value store with his unique keys as a project params:\n",
    "    * input_topic \n",
    "    * output_topic \n",
    "    * error_topic \n",
    "    * broker \n",
    "    * All the params assign to the project YAML\n",
    "3. Create a Redis service  - Run those commands:\n",
    "    * `helm repo add bitnami https://charts.bitnami.com/bitnami`\n",
    "    * `helm repo update`\n",
    "    * `helm install -n default-tenant  redis-test  --set auth.enabled=false bitnami/redis`\n",
    "4. Creating a redis service , saved as a project params with redis_path key - \n",
    "    * redis_path - redis://redis-test-master.default-tenant.svc.cluster.local:6379\n",
    "4. clone this repo to your jupyter service - make sure you are running this notebook from the repo directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "from mlrun import feature_store as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.datastore.targets import RedisNoSqlTarget, ParquetTarget\n",
    "from mlrun.feature_store.steps import OneHotEncoder, MapValues, DateExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1efff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d65ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'kafka-fs-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ef39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = mlrun.get_or_create_project(project_name,'./kafka_redis_fs/',user_project=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cc9b40",
   "metadata": {},
   "source": [
    "#### Creates Kafka Topics and Consumers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b307ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_in = project.get_param('input_topic')\n",
    "topic_out = project.get_param('output_topic')\n",
    "topic_err = project.get_param('error_topic')\n",
    "brokers = project.get_param('broker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180af277",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_admin_client = kafka.KafkaAdminClient(bootstrap_servers=brokers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd697d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_consumer_in = kafka.KafkaConsumer(topic_in,bootstrap_servers=brokers,auto_offset_reset=\"earliest\",max_poll_records=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ab774",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_consumer_out = kafka.KafkaConsumer(topic_out,bootstrap_servers=brokers,auto_offset_reset=\"earliest\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21edd8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_consumer_err = kafka.KafkaConsumer(topic_err,bootstrap_servers=brokers,auto_offset_reset=\"earliest\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa46d22",
   "metadata": {},
   "source": [
    "#### Creating FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_path = project.get_param('redis_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd44720",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = './data/data_ingest.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec73644",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_data = pd.read_csv(source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38c163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_data = transactions_data.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ac242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort value by time that the last time record will be the last row in the table\n",
    "transactions_data = transactions_data.sort_values(['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70525bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def len_device(event):\n",
    "    event['len_device']=len(event['device'])\n",
    "    return event\n",
    "\n",
    "def check_len_device(event):\n",
    "    if event['len_device'] > 5:\n",
    "        event['check_len_device'] = 'Bigger Then 5'\n",
    "        return event \n",
    "    else:\n",
    "        event['check_len_device'] = 'Smaller Or Equal to 5'\n",
    "        return event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b34c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun: end-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c70f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and add value mapping\n",
    "transaction_set = fs.FeatureSet(\"transactions\", \n",
    "                                 entities=[fs.Entity(\"source\")], \n",
    "                                 timestamp_key='timestamp', \n",
    "                                 description=\"transactions feature set\")\n",
    "main_categories = [\"es_transportation\", \"es_health\", \"es_otherservices\",\n",
    "       \"es_food\", \"es_hotelservices\", \"es_barsandrestaurants\",\n",
    "       \"es_tech\", \"es_sportsandtoys\", \"es_wellnessandbeauty\",\n",
    "       \"es_hyper\", \"es_fashion\", \"es_home\", \"es_contents\",\n",
    "       \"es_travel\", \"es_leisure\"]\n",
    "\n",
    "# One Hot Encode the newly defined mappings\n",
    "one_hot_encoder_mapping = {'category': main_categories,\n",
    "                           'gender': list(transactions_data.gender.unique())}\n",
    "\n",
    "# Define the graph steps\n",
    "transaction_set.graph\\\n",
    "    .to(DateExtractor(parts = ['hour', 'day_of_week'], timestamp_col = 'timestamp'))\\\n",
    "    .to(MapValues(mapping={'age': {'U': '0'}}, with_original_features=True))\\\n",
    "    .to(OneHotEncoder(mapping=one_hot_encoder_mapping)).respond()\n",
    "\n",
    "\n",
    "# Add aggregations for 2, 12, and 24 hour time windows\n",
    "transaction_set.add_aggregation(name='amount',\n",
    "                                column='amount',\n",
    "                                operations=['avg','sum', 'count','max'],\n",
    "                                windows=['2h', '12h', '24h'],\n",
    "                                period='1h')\n",
    "\n",
    "\n",
    "# Add the category aggregations over a 14 day window\n",
    "for category in main_categories:\n",
    "    transaction_set.add_aggregation(name=category,column=f'category_{category}',\n",
    "                                    operations=['count'], windows=['14d'], period='1d')\n",
    "\n",
    "# Add default (offline-parquet & online-nosql) targets\n",
    "targets = [RedisNoSqlTarget(path=redis_path),ParquetTarget()]\n",
    "transaction_set.set_targets(\n",
    "    targets=targets,\n",
    "    with_defaults=False,\n",
    ")\n",
    "# Plot the pipeline so we can see the different steps\n",
    "transaction_set.plot(rankdir=\"LR\", with_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d618f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_df = transaction_set.ingest(transactions_data,overwrite=True,infer_options=fs.InferOptions.default())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67304efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affbf569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check that all rows from the dataframe ingested\n",
    "ingest_df.shape[0]==transactions_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180f339b",
   "metadata": {},
   "source": [
    "#### Creating a deploy_ingestion_service function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de55d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = mlrun.code_to_function('steps',kind='serving',image='mlrun/mlrun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2425bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.datastore import KafkaSource\n",
    "source = KafkaSource(brokers=brokers,topics=topic_in)\n",
    "\n",
    "ingest_service = transaction_set.deploy_ingestion_service(source=source,run_config=fs.RunConfig(steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6a217",
   "metadata": {},
   "source": [
    "#### Test ingest Data with a simple requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a932122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import random\n",
    "df_json=pd.read_json('./json_files/json_49991.json',orient='index',typ='series')\n",
    "ingest_dict = df_json.to_dict()\n",
    "ingest_dict\n",
    "res=steps.invoke('/',ingest_dict)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3594367",
   "metadata": {},
   "source": [
    "#### Send requests to the Kafka trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b86664",
   "metadata": {},
   "outputs": [],
   "source": [
    "producer  = kafka.KafkaProducer(bootstrap_servers=[brokers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02972e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir('./json_files/') if isfile(join('./json_files/', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cfe35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles.remove('json_49991.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5559574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "s = datetime.datetime.now()\n",
    "import json\n",
    "for file in onlyfiles:\n",
    "    df_json=pd.read_json(f'./json_files/{file}',typ='series')\n",
    "    ingest_dict = df_json.to_dict()\n",
    "    ms=json.dumps(ingest_dict).encode('utf-8')\n",
    "    producer.send(topic=topic_in,value=ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e46b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in onlyfiles:\n",
    "    record = next(kafka_consumer_in)\n",
    "    counter += 1\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e42973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if all the inputs got into kafka\n",
    "counter == len(onlyfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd80b84",
   "metadata": {},
   "source": [
    "#### Creating feature vectore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c02f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"transactions.*\",\n",
    "]\n",
    "\n",
    "vector = fs.FeatureVector(\"transactions-vector\",features=features,description=\"this is my vector\")\n",
    "resp = vector.get_offline_features(with_indexes=True)\n",
    "# Preview the dataset\n",
    "resp.to_dataframe().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00220dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = vector.get_online_feature_service()\n",
    "resp = svc.get([{\"source\": 'C1145304322'}])\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_set.purge_targets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
