{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799025a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scenario2.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlrun.feature_store as fstore\n",
    "from mlrun.datastore import ParquetTarget,NoSqlTarget\n",
    "import mlrun\n",
    "from datetime import datetime, timedelta\n",
    "from mlrun.execution import MLClientCtx\n",
    "from sys import getsizeof\n",
    "\n",
    "data_size = 100\n",
    "\n",
    "def ingest_data(context : MLClientCtx, date:str, prefix:int):\n",
    "    cols = [\"col\" + str(x) for x in range(35)]\n",
    "    df = pd.DataFrame(np.random.random_sample((data_size, 35)), columns=cols)\n",
    "    ts = pd.Timestamp(date)\n",
    "    df['timestamp'] = ts\n",
    "    entity = [prefix + x for x in range(data_size)]\n",
    "    df['entity'] = entity\n",
    "    # each different feature set gets ingested the same dataset generated. Shape(data_size, 35)\n",
    "    \n",
    "    fs1 = fstore.get_feature_set(uri=\"store://feature-sets/ml2802/test1:latest\", \n",
    "                                project=\"ml2802\")\n",
    "    \n",
    "    fstore.ingest(fs1, df, overwrite=False)\n",
    "    \n",
    "    context.logger.info('fs1 ingested')\n",
    "    \n",
    "    fs2 = fstore.get_feature_set(uri=\"store://feature-sets/ml2802/test2:latest\", \n",
    "                                project=\"ml2802\")\n",
    "    \n",
    "    fstore.ingest(fs2, df, overwrite=False)\n",
    "    \n",
    "    context.logger.info('fs2 ingested')\n",
    "    \n",
    "    fs3 = fstore.get_feature_set(uri=\"store://feature-sets/ml2802/test3:latest\", \n",
    "                                project=\"ml2802\")\n",
    "    \n",
    "    fstore.ingest(fs3, df, overwrite=False)\n",
    "    \n",
    "    context.logger.info('fs3 ingested')\n",
    "    \n",
    "    fs4 = fstore.get_feature_set(uri=\"store://feature-sets/ml2802/test4:latest\", \n",
    "                                project=\"ml2802\")\n",
    "    \n",
    "    fstore.ingest(fs4, df, overwrite=False)\n",
    "    \n",
    "    context.logger.info('fs4 ingested')\n",
    "    \n",
    "    fs5 = fstore.get_feature_set(uri=\"store://feature-sets/ml2802/test5:latest\", \n",
    "                                project=\"ml2802\")\n",
    "    \n",
    "    fstore.ingest(fs5, df, overwrite=False)\n",
    "    \n",
    "    context.logger.info('fs5 ingested')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f0f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "project_name = 'ml2802'\n",
    "# Initialize the MLRun project object\n",
    "project = mlrun.get_or_create_project(project_name, context=\"./\", user_project=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b3062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun.feature_store as fstore\n",
    "from mlrun.datastore import ParquetTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57898881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 5 different FeatureSets\n",
    "test_set_1 = fstore.FeatureSet(\"test1\", \n",
    "                              entities=[fstore.Entity(\"entity\")], \n",
    "                              timestamp_key='timestamp', \n",
    "                              description=\"Test FS\")\n",
    "test_set_2 = fstore.FeatureSet(\"test2\", \n",
    "                              entities=[fstore.Entity(\"entity\")], \n",
    "                              timestamp_key='timestamp', \n",
    "                              description=\"Test FS\")\n",
    "test_set_3 = fstore.FeatureSet(\"test3\", \n",
    "                              entities=[fstore.Entity(\"entity\")], \n",
    "                              timestamp_key='timestamp', \n",
    "                              description=\"Test FS\")\n",
    "test_set_4 = fstore.FeatureSet(\"test4\", \n",
    "                              entities=[fstore.Entity(\"entity\")], \n",
    "                              timestamp_key='timestamp', \n",
    "                              description=\"Test FS\")\n",
    "test_set_5 = fstore.FeatureSet(\"test5\", \n",
    "                              entities=[fstore.Entity(\"entity\")], \n",
    "                              timestamp_key='timestamp', \n",
    "                              description=\"Test FS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f791da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "parq = ParquetTarget(name=\"part\", partitioned=True, time_partitioning_granularity=\"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97648335",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_1.set_targets(targets=[parq],with_defaults=False)\n",
    "test_set_2.set_targets(targets=[parq],with_defaults=False)\n",
    "test_set_3.set_targets(targets=[parq],with_defaults=False)\n",
    "test_set_4.set_targets(targets=[parq],with_defaults=False)\n",
    "test_set_5.set_targets(targets=[parq],with_defaults=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1540076",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_1.plot(rankdir=\"LR\", with_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_1.save()\n",
    "test_set_2.save()\n",
    "test_set_3.save()\n",
    "test_set_4.save()\n",
    "test_set_5.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_func = mlrun.code_to_function(name='test_func', kind='job', image='mlrun/mlrun', filename=\"scenario2.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6e7a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates={'2022-10-16':10000000 ,'2022-09-16':20000000,'2022-08-16': 30000000,'2022-07-16':40000000,'2022-06-16':50000000,'2022-05-16':60000000,'2022-04-16':70000000,'2022-03-16':80000000,\n",
    "       '2022-02-16':90000000,'2022-01-16':100000000,'2021-12-16':110000000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for every date, each time with different date and prefix\n",
    "\n",
    "for date in dates.keys():\n",
    "    test_func.run(name='ingest', \n",
    "                  handler='ingest_data',\n",
    "                  params = {'date' : date,\n",
    "                            'prefix' : dates[date]})\n",
    "    print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7981d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"test1.col0 as t1col0\",\n",
    "    \"test1.col1 as t1col1\",\n",
    "    \"test1.col2 as t1col2\",\n",
    "    \"test1.col3 as t1col3\",\n",
    "    \"test1.col4 as t1col4\",\n",
    "    \"test1.col5 as t1col5\",\n",
    "    \"test1.col6 as t1col6\",\n",
    "    \"test1.col7 as t1col7\",\n",
    "    \"test1.col8 as t1col8\",\n",
    "    \"test1.col9 as t1col9\",\n",
    "    \"test1.col10 as t1col10\",\n",
    "    \"test1.col11 as t1col11\",\n",
    "    \"test1.col12 as t1col12\",\n",
    "    \"test1.col13 as t1col13\",\n",
    "    \"test1.col14 as t1col14\",\n",
    "    \"test1.col15 as t1col15\",\n",
    "    \"test1.col16 as t1col16\",\n",
    "    \"test1.col17 as t1col17\",\n",
    "    \"test1.col18 as t1col18\",\n",
    "    \"test1.col19 as t1col19\",\n",
    "    \"test1.col20 as t1col20\",\n",
    "    \"test1.col21 as t1col21\",\n",
    "    \"test1.col22 as t1col22\",\n",
    "    \"test1.col23 as t1col23\",\n",
    "    \"test1.col24 as t1col24\",\n",
    "    \"test1.col25 as t1col25\",\n",
    "    \"test1.col26 as t1col26\",\n",
    "    \"test1.col27 as t1col27\",\n",
    "    \"test1.col28 as t1col28\",\n",
    "    \"test1.col29 as t1col29\",\n",
    "    \"test1.col30 as t1col30\",\n",
    "    \"test1.col31 as t1col31\",\n",
    "    \"test1.col32 as t1col32\",\n",
    "    \"test1.col33 as t1col33\",\n",
    "    \"test1.col34 as t1col34\",\n",
    "    \"test2.col0 as t2col0\",\n",
    "    \"test2.col1 as t2col1\",\n",
    "    \"test2.col2 as t2col2\",\n",
    "    \"test2.col3 as t2col3\",\n",
    "    \"test2.col4 as t2col4\",\n",
    "    \"test2.col5 as t2col5\",\n",
    "    \"test2.col6 as t2col6\",\n",
    "    \"test2.col7 as t2col7\",\n",
    "    \"test2.col8 as t2col8\",\n",
    "    \"test2.col9 as t2col9\",\n",
    "    \"test2.col10 as t2col10\",\n",
    "    \"test2.col11 as t2col11\",\n",
    "    \"test2.col12 as t2col12\",\n",
    "    \"test2.col13 as t2col13\",\n",
    "    \"test2.col14 as t2col14\",\n",
    "    \"test2.col15 as t2col15\",\n",
    "    \"test2.col16 as t2col16\",\n",
    "    \"test2.col17 as t2col17\",\n",
    "    \"test2.col18 as t2col18\",\n",
    "    \"test2.col19 as t2col19\",\n",
    "    \"test2.col20 as t2col20\",\n",
    "    \"test2.col21 as t2col21\",\n",
    "    \"test2.col22 as t2col22\",\n",
    "    \"test2.col23 as t2col23\",\n",
    "    \"test2.col24 as t2col24\",\n",
    "    \"test2.col25 as t2col25\",\n",
    "    \"test2.col26 as t2col26\",\n",
    "    \"test2.col27 as t2col27\",\n",
    "    \"test2.col28 as t2col28\",\n",
    "    \"test2.col29 as t2col29\",\n",
    "    \"test2.col30 as t2col30\",\n",
    "    \"test2.col31 as t2col31\",\n",
    "    \"test2.col32 as t2col32\",\n",
    "    \"test2.col33 as t2col33\",\n",
    "    \"test2.col34 as t2col34\",\n",
    "    \"test3.col0 as t3col0\",\n",
    "    \"test3.col1 as t3col1\",\n",
    "    \"test3.col2 as t3col2\",\n",
    "    \"test3.col3 as t3col3\",\n",
    "    \"test3.col4 as t3col4\",\n",
    "    \"test3.col5 as t3col5\",\n",
    "    \"test3.col6 as t3col6\",\n",
    "    \"test3.col7 as t3col7\",\n",
    "    \"test3.col8 as t3col8\",\n",
    "    \"test3.col9 as t3col9\",\n",
    "    \"test3.col10 as t3col10\",\n",
    "    \"test3.col11 as t3col11\",\n",
    "    \"test3.col12 as t3col12\",\n",
    "    \"test3.col13 as t3col13\",\n",
    "    \"test3.col14 as t3col14\",\n",
    "    \"test3.col15 as t3col15\",\n",
    "    \"test3.col16 as t3col16\",\n",
    "    \"test3.col17 as t3col17\",\n",
    "    \"test3.col18 as t3col18\",\n",
    "    \"test3.col19 as t3col19\",\n",
    "    \"test3.col20 as t3col20\",\n",
    "    \"test3.col21 as t3col21\",\n",
    "    \"test3.col22 as t3col22\",\n",
    "    \"test3.col23 as t3col23\",\n",
    "    \"test3.col24 as t3col24\",\n",
    "    \"test3.col25 as t3col25\",\n",
    "    \"test3.col26 as t3col26\",\n",
    "    \"test3.col27 as t3col27\",\n",
    "    \"test3.col28 as t3col28\",\n",
    "    \"test3.col29 as t3col29\",\n",
    "    \"test3.col30 as t3col30\",\n",
    "    \"test3.col31 as t3col31\",\n",
    "    \"test3.col32 as t3col32\",\n",
    "    \"test3.col33 as t3col33\",\n",
    "    \"test3.col34 as t3col34\",\n",
    "    \"test4.col0 as t4col0\",\n",
    "    \"test4.col1 as t4col1\",\n",
    "    \"test4.col2 as t4col2\",\n",
    "    \"test4.col3 as t4col3\",\n",
    "    \"test4.col4 as t4col4\",\n",
    "    \"test4.col5 as t4col5\",\n",
    "    \"test4.col6 as t4col6\",\n",
    "    \"test4.col7 as t4col7\",\n",
    "    \"test4.col8 as t4col8\",\n",
    "    \"test4.col9 as t4col9\",\n",
    "    \"test4.col10 as t4col10\",\n",
    "    \"test4.col11 as t4col11\",\n",
    "    \"test4.col12 as t4col12\",\n",
    "    \"test4.col13 as t4col13\",\n",
    "    \"test4.col14 as t4col14\",\n",
    "    \"test4.col15 as t4col15\",\n",
    "    \"test4.col16 as t4col16\",\n",
    "    \"test4.col17 as t4col17\",\n",
    "    \"test4.col18 as t4col18\",\n",
    "    \"test4.col19 as t4col19\",\n",
    "    \"test4.col20 as t4col20\",\n",
    "    \"test4.col21 as t4col21\",\n",
    "    \"test4.col22 as t4col22\",\n",
    "    \"test4.col23 as t4col23\",\n",
    "    \"test4.col24 as t4col24\",\n",
    "    \"test4.col25 as t4col25\",\n",
    "    \"test4.col26 as t4col26\",\n",
    "    \"test4.col27 as t4col27\",\n",
    "    \"test4.col28 as t4col28\",\n",
    "    \"test4.col29 as t4col29\",\n",
    "    \"test4.col30 as t4col30\",\n",
    "    \"test4.col31 as t4col31\",\n",
    "    \"test4.col32 as t4col32\",\n",
    "    \"test4.col33 as t4col33\",\n",
    "    \"test4.col34 as t4col34\",\n",
    "    \"test5.*\",\n",
    "]\n",
    "\n",
    "fv = fstore.FeatureVector(\"test-vector-big\",features=features,description=\"this is my vector\")\n",
    "fv.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277acdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates={'2022-10-16':3330000 ,'2022-09-16':3330000,'2022-08-16': 3330000,'2022-07-16':3330000,'2022-06-16':3330000,'2022-05-16':3330000,'2022-04-16':3330000,'2022-03-16':3330000,\n",
    "       '2022-02-16':3330000,'2022-01-16':3330000,'2021-12-16':3330000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51646a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for every date, each time with different date and prefix\n",
    "\n",
    "\n",
    "for date in dates.keys():\n",
    "    test_func.run(name='ingest', \n",
    "                  handler='ingest_data',\n",
    "                  params = {'date' : date,\n",
    "                            'prefix' : dates[date]})\n",
    "    print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a5d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import feature_store as fstore\n",
    "from mlrun.runtimes import Spark3Runtime\n",
    "from datetime import datetime, timedelta\n",
    "from mlrun.feature_store import RunConfig\n",
    "from mlrun.datastore import ParquetTarget\n",
    "from mlrun import auto_mount\n",
    "import mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Spark3Runtime.deploy_default_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile spark-read.py\n",
    "import mlrun\n",
    "from mlrun.feature_store.retrieval import SparkFeatureMerger\n",
    "from mlrun.datastore.targets import get_target_driver\n",
    "\n",
    "def spark_handler(context, vector_uri, target, entity_rows=None, \n",
    "                  timestamp_column=None, drop_columns=None, with_indexes=None, query=None):\n",
    "    vector = context.get_store_resource(vector_uri)\n",
    "    store_target = get_target_driver(target, vector)\n",
    "    entity_timestamp_column = timestamp_column or vector.spec.timestamp_field\n",
    "    if entity_rows:\n",
    "        entity_rows = entity_rows.as_df()\n",
    "\n",
    "    context.logger.info(f\"starting vector merge task to {vector.uri}\")\n",
    "    merger = SparkFeatureMerger(vector)\n",
    "    resp = merger.start(entity_rows, entity_timestamp_column, store_target, drop_columns, with_indexes=with_indexes, \n",
    "                        query=query)\n",
    "    target = vector.status.targets[store_target.name].to_dict()\n",
    "    context.log_result('feature_vector', vector.uri)\n",
    "    context.log_result('target', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a003db",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_func = mlrun.code_to_function(name=\"spark-read\",\n",
    "                                    kind=\"spark\",\n",
    "                                    handler=\"spark_handler\",\n",
    "                                    filename=\"spark-read.py\",\n",
    "                                    image=\".spark-job-default-image\").apply(auto_mount())\n",
    "spark_func.with_executor_requests(cpu=\"1\",mem=\"1G\")\n",
    "spark_func.with_driver_requests(cpu=\"1\",mem=\"1G\")\n",
    "spark_func.with_driver_limits(cpu=\"1\")\n",
    "spark_func.with_executor_limits(cpu=\"1\")\n",
    "spark_func.with_igz_spark()\n",
    "spark_func.spec.image_pull_policy = \"Always\"\n",
    "spark_func.spec.replicas = 3\n",
    "rc = RunConfig(spark_func, local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07fe829",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_func.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047fb5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = fv.get_offline_features(with_indexes=True,\n",
    "                               timestamp_for_filtering='timestamp',\n",
    "                               start_time = datetime.strptime(\"2021-01-16\", '%Y-%m-%d')-timedelta(days=1),\n",
    "                               end_time = datetime.strptime(\"2022-10-16\", '%Y-%m-%d'),\n",
    "                               engine='spark',\n",
    "                               run_config=rc,\n",
    "                               target=ParquetTarget(path=\"v3io:///bigdata/test_parq\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b1798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998544fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/v3io/bigdata/test_parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74501087",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName(\"parquetFile\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb17ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"v3io://bigdata/test_parq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a49eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"ParquetTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5e33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSQL = spark.sql(\"select count(*) from ParquetTable where limit 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d8e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSQL.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
