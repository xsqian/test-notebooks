{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837cc32-505b-4129-9641-13d816979780",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d35479-7453-4ec9-84ee-686d7f779561",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64435f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import mlrun\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da3227-83ec-443a-81d8-d7aaa2ac9bd1",
   "metadata": {},
   "source": [
    "# MLflow Tracker test notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131a4b2-7621-4029-b594-7c35ed4ef617",
   "metadata": {},
   "source": [
    "### First let's create the project and the context folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae06d43-623f-4bbe-a49c-4c0e42700eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name1 = \"test-mlflow-tracking1\"\n",
    "# we choose the first run option from above\n",
    "mlrun.mlconf.external_platform_tracking.mlflow.match_experiment_to_runtime = True\n",
    "\n",
    "# Create a project for this demo:\n",
    "project = mlrun.get_or_create_project(name=project_name1, context=\"./test_mlflow_tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8db175-51f4-4218-afd1-752cc0e65216",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Xgboost example function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1e133-954d-47a3-9b0f-6e181fe12ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ./test_mlflow_tracking/training.py\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "from mlflow import log_metric\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def example_xgb_run():\n",
    "    # prepare train and test data\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # enable auto logging\n",
    "    mlflow.xgboost.autolog()\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # train model\n",
    "        params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": 3,\n",
    "            \"learning_rate\": 0.3,\n",
    "            \"eval_metric\": \"mlogloss\",\n",
    "            \"colsample_bytree\": 1.0,\n",
    "            \"subsample\": 1.0,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        model = xgb.train(params, dtrain, evals=[(dtrain, \"train\")])\n",
    "        \n",
    "        # evaluate model\n",
    "        y_proba = model.predict(dtest)\n",
    "        y_pred = y_proba.argmax(axis=1)\n",
    "        loss = log_loss(y_test, y_proba)\n",
    "        acc = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf984c9-78a9-443f-9465-111263101dcd",
   "metadata": {},
   "source": [
    "## Mlrun code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e4b39-9f39-40ae-aac4-7c4f42bce9bd",
   "metadata": {},
   "source": [
    "### Need to change the mlrun config in order to use the tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b194d04-e08f-4161-a65b-4f18d10fdbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlrun.mlconf.external_platform_tracking.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16bb4db-8a2a-4453-a42e-0e8e74ab8f53",
   "metadata": {},
   "source": [
    "### 3 possible way to run tracking:\n",
    "1. We can set: 'mlrun.mlconf.external_platform_tracking.mlflow.match_experiment_to_runtime' to True, this determines the run id and is the safest way\n",
    "2. We can set the experiment name at: 'mlflow.environment_variables.MLFLOW_EXPERIMENT_NAME.set', this determines the experiment and we track the run added to it\n",
    "3. We can just run it, then we will look across all experiments for added runs, this is not encouraged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7bc72a-bd1b-408a-afa8-e474d91c4a20",
   "metadata": {},
   "source": [
    "### Then we create the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3382b909-a8dc-41a3-afb1-b64df9bb7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function_name = \"example-xgb-run\"\n",
    "handler_name = \"example_xgb_run\"\n",
    "\n",
    "# Create a MLRun function using the example train file (all the functions must be located in it):\n",
    "training_func = project.set_function(\n",
    "    func=\"training.py\",\n",
    "    name=function_name,\n",
    "    kind=\"job\",\n",
    "    image=\"mlrun/mlrun\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91597f57-364d-4d2a-b926-97b9d8afc81b",
   "metadata": {},
   "source": [
    "### Now we run the function, and after that we can look at the UI and see all metrics and parameters are logged in mlrun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba452dd-1756-4bfb-af64-d741e234dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we run the example code using mlrun\n",
    "train_run = training_func.run(\n",
    "    local=True, handler=handler_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc67ea6-500c-474a-bf33-da09b3b92e19",
   "metadata": {},
   "source": [
    "## Now let's check that the artifacts were correctly created in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b727f77-4d96-4e54-938f-47da33de40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_iter_from_uri(uri):\n",
    "    return re.sub(r\"#\\d+:latest\", '', uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bca74b-c71a-4b1c-85a5-f7957ca614e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler_name = handler_name.replace(\"_\", \"-\")\n",
    "artifact_prefix = function_name + \"-\" + handler_name + \"_\"\n",
    "\n",
    "feature_importance_weights_json = project.get_artifact(artifact_prefix + \"feature_importance_weight_json\", tag=\"latest\")\n",
    "feature_importance_weights_png = project.get_artifact(artifact_prefix + \"feature_importance_weight_png\", tag=\"latest\")\n",
    "feature_importance_weights_json_uri = feature_importance_weights_json.uri.replace(\"#0\", \"\")\n",
    "feature_importance_weights_png_uri= feature_importance_weights_png.uri.replace(\"#0\", \"\")\n",
    "model = project.list_models(name=artifact_prefix+'model', tag=\"latest\", best_iteration=True)[0]\n",
    "\n",
    "assert feature_importance_weights_json_uri\n",
    "assert feature_importance_weights_png_uri\n",
    "assert model.uri\n",
    "assert feature_importance_weights_json_uri==train_run.outputs[\"feature_importance_weight_json\"]\n",
    "assert feature_importance_weights_png_uri==train_run.outputs[\"feature_importance_weight_png\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844edc05-0b6a-4e84-9213-1d3cbf6f833e",
   "metadata": {},
   "source": [
    "# Now we will test this as a model server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6489cb-35e2-4b0d-8a0d-e8954a44534a",
   "metadata": {},
   "source": [
    "### To use this as an model server we need to implement two functions, load and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381bb171-5111-4cc1-b81c-fdb813a6208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./test_mlflow_tracking/serving.py\n",
    "\n",
    "import zipfile\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import mlrun\n",
    "from mlrun.serving.v2_serving import V2ModelServer\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "\n",
    "class MLFlowModelServer(V2ModelServer):\n",
    "    \"\"\"\n",
    "    MLFlow tracker Model serving class, inheriting the V2ModelServer class for being initialized automatically by the model\n",
    "    server and be able to run locally as part of a nuclio serverless function, or as part of a real-time pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        loads an model that was logged by the MLFlow tracker model\n",
    "        \"\"\"\n",
    "        # all we need to do is unzip the model dir and then use mlflow's load function\n",
    "        model_file, _ = self.get_model(\".zip\")\n",
    "        model_path_unzip = model_file.replace(\".zip\", \"\")\n",
    "\n",
    "        with zipfile.ZipFile(model_file, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(model_path_unzip)\n",
    "            \n",
    "        self.model = mlflow.pyfunc.load_model(model_path_unzip)\n",
    "\n",
    "    def predict(self, request: Dict[str, Any]) -> list:\n",
    "        \"\"\"\n",
    "        Infer the inputs through the model. The inferred data will\n",
    "        be read from the \"inputs\" key of the request.\n",
    "\n",
    "        :param request: The request to the model using xgboost's predict. \n",
    "                The input to the model will be read from the \"inputs\" key.\n",
    "\n",
    "        :return: The model's prediction on the given input.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the inputs and set to accepted type:\n",
    "        inputs = pd.DataFrame(request[\"inputs\"])\n",
    "\n",
    "        # Predict using the model's predict function:\n",
    "        predictions = self.model.predict(inputs)\n",
    "\n",
    "        # Return as list:\n",
    "        return predictions.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40182a6f-fc46-4a33-a7f5-7ee8ee171966",
   "metadata": {},
   "source": [
    "### creating the server and serving function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe910b-e177-4af7-84de-41a571d1774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_name = \"example-xgb-server\"\n",
    "serving_func = project.set_function(\n",
    "    func=\"serving.py\",\n",
    "    name=\"example-xgb-server\",\n",
    "    kind=\"serving\",\n",
    "    image=\"mlrun/mlrun\",\n",
    "    requirements=[\"xgboost\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfd48f-a90e-4fe6-9caa-ddffeacf63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the model\n",
    "serving_func.add_model(\"mlflow_xgb_model\", class_name=\"MLFlowModelServer\", model_path=train_run.outputs[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d7c06-4972-4881-9bc9-fba7db0adbe4",
   "metadata": {},
   "source": [
    "### Let's try to test our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e546e92-34ff-4eee-beb7-4afa391b3626",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_func.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe912817-9bfb-49d9-bc81-5f6114dd7892",
   "metadata": {},
   "source": [
    "## Now let's check that the serving function was correctly created in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc22af-ccd3-4052-b3fc-6b67a3346045",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = project.get_function(function_name)\n",
    "assert func\n",
    "assert func==serving_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673c1a3d-f343-4277-a066-a66e657c50f3",
   "metadata": {},
   "source": [
    "# Offline tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78128565-d17f-40fb-be55-2a37aaf970c7",
   "metadata": {},
   "source": [
    "in this example we will run a function that's being logged by mlflow without mlrun,\n",
    "and then import it into mlrun afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3c9a9b-9b5b-4f56-bbf2-03f8a0c0122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./test_mlflow_tracking/offline_training.py\n",
    "\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "from mlflow import log_metric\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mlflow.environment_variables.MLFLOW_EXPERIMENT_NAME.set(\"example_xgb_run\")\n",
    "\n",
    "# the function we run that is being logged by mlflow\n",
    "def example_xgb_run():\n",
    "    # prepare train and test data\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # enable auto logging\n",
    "    mlflow.xgboost.autolog()\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    with mlflow.start_run(run_name=\"offline-import-run\"):\n",
    "        # train model\n",
    "        params = {\n",
    "            \"objective\": \"multi:softprob\",\n",
    "            \"num_class\": 3,\n",
    "            \"learning_rate\": 0.3,\n",
    "            \"eval_metric\": \"mlogloss\",\n",
    "            \"colsample_bytree\": 1.0,\n",
    "            \"subsample\": 1.0,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        model = xgb.train(params, dtrain, evals=[(dtrain, \"train\")])\n",
    "        \n",
    "        # evaluate model\n",
    "        y_proba = model.predict(dtest)\n",
    "        y_pred = y_proba.argmax(axis=1)\n",
    "        loss = log_loss(y_test, y_proba)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # log metrics by hand\n",
    "        mlflow.log_metrics({\"log_loss\": loss, \"accuracy\": acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b4ee0-5d8d-4bc6-939c-1614555dd840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\"./\"))\n",
    "from test_mlflow_tracking.offline_training import example_xgb_run\n",
    "import mlrun\n",
    "from mlrun.track.trackers.mlflow_tracker import MLFlowTracker\n",
    "import tempfile\n",
    "import mlflow\n",
    "# Allow all tracking\n",
    "mlrun.mlconf.external_platform_tracking.enabled = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cdf546-0afd-4806-ac09-31c246dc08c3",
   "metadata": {},
   "source": [
    "## Import offline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f509c3c7-1ceb-46ff-80d7-dc78003eb928",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name2 = \"test-mlflow-tracking2\"\n",
    "\n",
    "# Create a project for this demo:\n",
    "project = mlrun.get_or_create_project(name=project_name2, context=\"./test_mlflow_tracking\")\n",
    "\n",
    "# Create a MLRun function that we will log in to:\n",
    "function_name = \"example-xgb-run-offline\"\n",
    "\n",
    "training_func = project.set_function(\n",
    "    func=\"offline_training.py\",\n",
    "    name=function_name,\n",
    "    kind=\"job\",\n",
    "    image=\"mlrun/mlrun\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab00ead-5ce7-4b42-ab2f-6f3c1006c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a temporary working area to avoid junk in ours    \n",
    "with tempfile.TemporaryDirectory() as test_directory:\n",
    "    mlflow.set_tracking_uri(test_directory)  # Tell mlflow where to save logged data\n",
    "\n",
    "    # Run mlflow wrapped code\n",
    "    example_xgb_run()\n",
    "\n",
    "    # Set mlconf path to artifacts\n",
    "    mlrun.mlconf.artifact_path = test_directory + \"/artifact\"\n",
    "    \n",
    "    # Find last ran mlflow run\n",
    "    mlflow_run = mlflow.last_active_run()\n",
    "    \n",
    "    # Import the run into mlrun using the function we created earlier\n",
    "    imported_run = MLFlowTracker().import_run(\n",
    "        project=project,\n",
    "        reference_id=mlflow_run.info.run_id,\n",
    "        function_name=function_name,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b94ac-4ad2-47c3-b97b-3d9cc4ad10e3",
   "metadata": {},
   "source": [
    "## Checking the run has been registred in the project and the artifacts created\n",
    "Cannot check more than that since the runs/artifact names is auto-generated by mlflow and we currently don't support setting a name when importing an offline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb14ffe-d080-43f7-8892-77e8771a4acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = project.list_runs()\n",
    "assert runs\n",
    "assert len(runs)==1\n",
    "\n",
    "artifacts = project.list_artifacts()\n",
    "assert artifacts\n",
    "assert len(artifacts)==3\n",
    "assert artifacts[0]['metadata']['key']=='feature_importance_weight_json'\n",
    "assert artifacts[1]['metadata']['key']=='feature_importance_weight_png'\n",
    "assert artifacts[2]['metadata']['key']=='model'\n",
    "\n",
    "models = project.list_models()\n",
    "assert models\n",
    "assert len(models)==1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76eeed4-280c-4fe7-b573-888d18438167",
   "metadata": {},
   "source": [
    "## Import offline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a350e2-bedf-4da3-8822-6cc77064cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name3 = \"test-mlflow-tracking3\"\n",
    "\n",
    "# We create a temporary working area to avoid junk in ours    \n",
    "with tempfile.TemporaryDirectory() as test_directory:\n",
    "    mlflow.set_tracking_uri(test_directory)  # Tell mlflow where to save logged data\n",
    "\n",
    "    # Run mlflow wrapped code\n",
    "    example_xgb_run()\n",
    "    \n",
    "    # Create a project for this tester:\n",
    "    project = mlrun.get_or_create_project(name=project_name3, context=test_directory)\n",
    "\n",
    "    # Access model's uri through mlflow's last run\n",
    "    mlflow_run = mlflow.last_active_run()\n",
    "    model_uri = mlflow_run.info.artifact_uri + \"/model\"\n",
    "\n",
    "    key = \"test_model\"\n",
    "    MLFlowTracker().import_model(\n",
    "        project=project,\n",
    "        reference_id=model_uri,\n",
    "        key=key,\n",
    "        metrics=mlflow_run.data.metrics,\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed05b10-7527-44e7-8d49-1d21d37dd0af",
   "metadata": {},
   "source": [
    "## Checking if the model was logged into project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe0f9c-9f9c-44aa-8a6e-293156529a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate model was logged into project\n",
    "assert project.get_artifact(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ccb1b6-819d-4a3e-a9b2-d21b4bd39f41",
   "metadata": {},
   "source": [
    "## Now we test all the different mlflow logging options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a158b-e457-47af-9a6c-3284af919727",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./test_mlflow_tracking/log_stuff.py\n",
    "\n",
    "import json\n",
    "import os\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly import graph_objects as go\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "mlflow.environment_variables.MLFLOW_EXPERIMENT_NAME.set(\"log_stuff\")\n",
    "\n",
    "\n",
    "def log_stuff():\n",
    "    \n",
    "    print(\"hey it's amit\")\n",
    "# ---------------------------------------------option 1------------------------------------------------\n",
    "    # txt file to log\n",
    "    features = \"feature1, feature2, feature3, feature4, feature5\"\n",
    "    with open(\"features.txt\", \"w\") as f:\n",
    "        f.write(features)\n",
    "        \n",
    "# ---------------------------------------------option 2------------------------------------------------    \n",
    "    # directory with txt/json files to log\n",
    "    features = \"dir_feature1, dir_feature2, dir_feature3, dir_feature4, dir_feature5\"\n",
    "    data = {\"json_feature1\": \"val1\", \"json_feature2\": 22222, \"json_feature3\": True}\n",
    "\n",
    "    # Create couple of artifact files under the directory \"data\"\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    with open(\"data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    with open(\"features.txt\", \"w\") as f:\n",
    "        f.write(features)\n",
    "        \n",
    "# ---------------------------------------------option 3------------------------------------------------\n",
    "    \n",
    "    matplotlib_fig, ax = plt.subplots()\n",
    "    ax.plot([0, 1], [2, 3])\n",
    "    \n",
    "    plotly_fig = go.Figure(go.Scatter(x=[0, 1], y=[2, 3]))\n",
    "    \n",
    "# ---------------------------------------------option 4------------------------------------------------\n",
    "    \n",
    "    np_image = np.random.randint(0, 256, size=(100, 100, 3), dtype=np.uint8)\n",
    "    PIL_image = Image.new(\"RGB\", (100, 100))\n",
    "    \n",
    "# ---------------------------------------------option 5------------------------------------------------\n",
    "    \n",
    "    array = np.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "    np_dataset = mlflow.data.from_numpy(array)\n",
    "    \n",
    "    \n",
    "    data = {\n",
    "        'Name': ['John', 'Alice', 'Bob', 'Charlie'],\n",
    "        'Age': [25, 28, 22, 30],\n",
    "        'City': ['New York', 'San Francisco', 'Seattle', 'Boston']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    pd_dataset = mlflow.data.from_pandas(df)\n",
    "    \n",
    "# ---------------------------------------------option 6------------------------------------------------\n",
    "    table_dict = {\n",
    "        \"inputs\": [\"What is MLflow?\", \"What is Databricks?\"],\n",
    "        \"outputs\": [\"MLflow is ...\", \"Databricks is ...\"],\n",
    "        \"toxicity\": [0.0, 0.0],\n",
    "    }\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    with mlflow.start_run():\n",
    "# ---------------------------------------------log option 1------------------------------------------------\n",
    "        # log a txt file as an artifact\n",
    "        mlflow.log_artifact(\"features.txt\")\n",
    "        \n",
    "# ---------------------------------------------log option 2------------------------------------------------\n",
    "        # log the files in a directory\n",
    "        mlflow.log_artifacts(\"data\", artifact_path=\"states\")\n",
    "        \n",
    "# ---------------------------------------------log option 3------------------------------------------------\n",
    "        # log matplotlib/plotly figures\n",
    "        mlflow.log_figure(matplotlib_fig, \"matplotlib_figure.png\")\n",
    "        mlflow.log_figure(plotly_fig, \"plotly_figure.html\")\n",
    "        \n",
    "# ---------------------------------------------log option 4------------------------------------------------\n",
    "        # log np/PIL images\n",
    "        mlflow.log_image(np_image, \"np_image.png\")\n",
    "        mlflow.log_image(PIL_image, \"PIL_image.png\")\n",
    "        \n",
    "# ---------------------------------------------log option 5------------------------------------------------\n",
    "        # log numpy/pandas dataset\n",
    "        mlflow.log_input(np_dataset, context=\"numpy_training\")\n",
    "        mlflow.log_input(pd_dataset, context=\"pandas_training\")\n",
    "    \n",
    "# ---------------------------------------------log option 6------------------------------------------------    \n",
    "        # log a dict/dataframe as a table artifact (json format)\n",
    "        mlflow.log_table(data=table_dict, artifact_file=\"dict_table.json\")\n",
    "        mlflow.log_table(data=df, artifact_file=\"df_table.json\")\n",
    "    \n",
    "    \n",
    "    \n",
    "        # log string \n",
    "        mlflow.log_text(\"text1\", \"file1.txt\")\n",
    "        \n",
    "        # log dicts as json/yaml\n",
    "        mlflow.log_dict({\"json_key1\":\"json_val1\"}, \"data.json\")\n",
    "        mlflow.log_dict({\"yaml_key1\":\"yaml_val1\"}, \"data.yaml\")\n",
    "        \n",
    "        # log a numeric value\n",
    "        mlflow.log_metric(key=\"metric_key\", value=12345)\n",
    "        mlflow.log_metrics({\"metric_key1\": 1, \"metric_key2\": 2})\n",
    "        \n",
    "        # log a string parameter\n",
    "        mlflow.log_param(key=\"param_key\", value=\"param_val\")\n",
    "        mlflow.log_params({\"param_key1\":\"param_val1\", \"param_key2\":\"param_val2\"})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75dc24-eaad-4fff-ad20-706d62146e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlrun.mlconf.external_platform_tracking.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c60e32-c8d4-430c-8273-406a451ecf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name4 = \"test-mlflow-tracking4\"\n",
    "\n",
    "# we choose the first run option from above\n",
    "mlrun.mlconf.external_platform_tracking.mlflow.match_experiment_to_runtime = True\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "\n",
    "\n",
    "project = mlrun.get_or_create_project(name=project_name4, context=\"./test_mlflow_tracking\")\n",
    "function_name = \"log-stuff-func\"\n",
    "handler_name = \"log_stuff\"\n",
    "\n",
    "# Create a MLRun function\n",
    "loging_func = project.set_function(\n",
    "    func=\"log_stuff.py\",\n",
    "    name=function_name,\n",
    "    kind=\"job\",\n",
    "    image=\"mlrun/mlrun\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72b216-dc6c-45a9-9ca8-2f2d096b99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "log_run = loging_func.run(\n",
    "    local=True, handler=handler_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c7f40-107a-4adb-9543-7e917dc75d00",
   "metadata": {},
   "source": [
    "## Checking the logged artifacts/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf89ca5b-af5b-4038-b843-90a1f33628da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the artifacts and the runs from the project\n",
    "\n",
    "artifacts = project.list_artifacts()\n",
    "artifact_keys = [art['metadata']['key'] for art in artifacts]\n",
    "\n",
    "runs = project.list_runs()\n",
    "results = runs[0]['status']['results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6664a-ad15-400d-a9b0-fd7b82dc873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler_name = handler_name.replace(\"_\", \"-\")\n",
    "artifact_prefix = function_name + \"-\" + handler_name + \"_\"\n",
    "\n",
    "assert artifacts\n",
    "assert len(artifacts)==11\n",
    "\n",
    "for key in artifact_keys:\n",
    "    # Check artifacts from the project exist in the run outputs\n",
    "    assert key in log_run.outputs.keys()\n",
    "    \n",
    "    # Now checks the values are the same (store uri)\n",
    "    artifact_uri = project.get_artifact(key=artifact_prefix+key).uri\n",
    "    artifact_uri = artifact_uri.replace(\"#0\", \"\")\n",
    "    assert artifact_uri==log_run.outputs[key]\n",
    "    \n",
    "# Check the logged results (metrics)\n",
    "for result in results.keys():\n",
    "    assert result in log_run.outputs.keys()\n",
    "    assert results[result]==log_run.outputs[result]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9d0e4-a2f8-47ef-b7b1-4a5607e68952",
   "metadata": {},
   "source": [
    "## Deleting projects and relevant resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687102c7-02cb-48a9-8ac6-5e2d8a59e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_db = mlrun.get_run_db()\n",
    "run_db.delete_project(project_name1, mlrun.common.schemas.constants.DeletionStrategy.cascade)\n",
    "run_db.delete_project(project_name2, mlrun.common.schemas.constants.DeletionStrategy.cascade)\n",
    "run_db.delete_project(project_name3, mlrun.common.schemas.constants.DeletionStrategy.cascade)\n",
    "run_db.delete_project(project_name4, mlrun.common.schemas.constants.DeletionStrategy.cascade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd577f-f356-47fb-a86f-0bc0cb5cfe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "files_to_delete = [\"data.json\", \"features.txt\"]\n",
    "for file in files_to_delete:\n",
    "    try:\n",
    "        os.remove(file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting file: {e}\")\n",
    "\n",
    "\n",
    "folders = [\"test_mlflow_tracking\", \"data\", \"mlruns\"]\n",
    "\n",
    "for folder_name in folders:\n",
    "    try:\n",
    "        shutil.rmtree(folder_name)\n",
    "        print(f\"Folder '{folder_name}' deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting folder: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
