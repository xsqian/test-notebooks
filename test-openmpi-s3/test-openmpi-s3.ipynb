{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0ef1fb",
   "metadata": {},
   "source": [
    "# Loading And Storing Data From And Into S3 Running Open-MPI Job\n",
    "\n",
    "This test generates a chunk of data, uploading it to S3 and then process it in a `job` and in a `mpijob`. Later it will verify that:\n",
    "\n",
    "* The data was handled properly and results were equal.\n",
    "* The stored dataset artifact in S3 is loadable and equal.\n",
    "* The mpijob run was faster (only possible on big data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d098ffe",
   "metadata": {},
   "source": [
    "## General Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6edd6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b014ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "from utils import S3Client\n",
    "\n",
    "# AWS Credentials:\n",
    "AWS_ACCESS_KEY_ID = os.environ.get(\"AWS_ACCESS_KEY_ID\", \"\")\n",
    "AWS_SECRET_ACCESS_KEY = os.environ.get(\"AWS_SECRET_ACCESS_KEY\", \"\")\n",
    "assert AWS_ACCESS_KEY_ID != \"\" and AWS_SECRET_ACCESS_KEY != \"\" \n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "\n",
    "# Path to store the generated data:\n",
    "LOCAL_DATA_PATH = \"./data\"\n",
    "S3_BUCKET = os.environ.get(\"S3_BUCKET\", \"testbucket-igz-temp\")\n",
    "S3_PROJECT_DIRECTORY = \"test-mpijob-s3-{}\".format(str(random.randint(0,10000)))\n",
    "DATA_PATH = \"data_{}\".format(str(random.randint(0,10000)))\n",
    "S3_DATA_PATH = os.path.join(S3_PROJECT_DIRECTORY, DATA_PATH)\n",
    "\n",
    "# Number of samples of generated data (number of rows in the data table):\n",
    "N_SAMPLES = 10_000\n",
    "\n",
    "# Number of features of the generated data (number of columns in the data table):\n",
    "N_FEATURES = 10\n",
    "\n",
    "# The amount of ranks to deploy for the open mpi job (used for parquet partitions of the generated data):\n",
    "N_RANKS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16072a29",
   "metadata": {},
   "source": [
    "## 1. Generate Data:\n",
    "\n",
    "1. Generate random data.\n",
    "2. Turn the data into a `pandas.DataFrame` naming the columns `features_{i}` and adding the partioting column (year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6cf26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_data(\n",
    "    output_path: str,\n",
    "    n_samples: int, \n",
    "    n_features: int, \n",
    "    n_partitions: int,\n",
    "):\n",
    "    # Generate data:\n",
    "    data = np.random.random(size=(n_samples, n_features))\n",
    "    \n",
    "    # Create a dataframe:\n",
    "    data = pd.DataFrame(\n",
    "        data=data, \n",
    "        columns=[f\"feature_{i}\" for i in range(n_features)]\n",
    "    )\n",
    "    data[\"year\"] = np.random.randint(2000, 2000 + n_partitions, size=n_samples)\n",
    "    \n",
    "    # Save to parquets:\n",
    "    data.to_parquet(output_path, partition_cols=[\"year\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdff9316",
   "metadata": {},
   "source": [
    "Generate the data (will require writing permissions to the local directory and of course to S3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27dbefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete past generated data (in case there was a past failure):\n",
    "if os.path.exists(LOCAL_DATA_PATH):\n",
    "    shutil.rmtree(os.path.abspath(LOCAL_DATA_PATH))\n",
    "\n",
    "# Generate new data:\n",
    "generate_data(\n",
    "    output_path=LOCAL_DATA_PATH,\n",
    "    n_samples=N_SAMPLES, \n",
    "    n_features=N_FEATURES, \n",
    "    n_partitions=N_RANKS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac2408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the S3 client:\n",
    "s3_client = S3Client(\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    ")\n",
    "\n",
    "# Delete the project and data in S3 (in case there was a past failure):\n",
    "try:\n",
    "    s3_client.delete(\n",
    "        bucket=S3_BUCKET,\n",
    "        s3_path=S3_PROJECT_DIRECTORY,\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "# Upload it to S3:\n",
    "s3_client.upload(\n",
    "    bucket=S3_BUCKET,\n",
    "    local_path=LOCAL_DATA_PATH,\n",
    "    s3_path=S3_DATA_PATH,\n",
    "    replace=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aeb6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete new generated data (data will be loaded from S3):\n",
    "shutil.rmtree(os.path.abspath(LOCAL_DATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76166f",
   "metadata": {},
   "source": [
    "## 2. Data Processing Code\n",
    "\n",
    "1. Read the data into a pandas (dask) `DataFrame` using MLRun's `DataItem.as_df`'s method.\n",
    "2. Do some calculations.\n",
    "\n",
    "The calculations are accumulated into a single value that will be logged as a result along a single column of data (means in this case) to be stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8d757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff13dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mlrun\n",
    "\n",
    "\n",
    "def process_data(context: mlrun.MLClientCtx, data_path: mlrun.DataItem):\n",
    "    # Start the timer:\n",
    "    run_time = time.time()\n",
    "    \n",
    "    # Check if 'job' or 'mpijob':\n",
    "    is_mpijob = context.labels.get(\"kind\", \"\") == \"mpijob\"\n",
    "    \n",
    "    # Get MPI rank:\n",
    "    comm = None\n",
    "    rank = 0\n",
    "    if is_mpijob:\n",
    "        from mpi4py import MPI\n",
    "        comm = MPI.COMM_WORLD\n",
    "        rank = comm.Get_rank()\n",
    "    \n",
    "    # Get the data:\n",
    "    if is_mpijob:\n",
    "        # Set path to the rank's part in the data partition:\n",
    "        data_path._url = os.path.join(data_path._url, f\"year={2000 + rank}\")\n",
    "    data = data_path.as_df(format=\"parquet\")\n",
    "    \n",
    "    # Do some random calculations:\n",
    "    result = 0\n",
    "    for column in data.columns:\n",
    "        if column == \"year\":\n",
    "            continue\n",
    "        for _, value in data[column].items():\n",
    "            result += value\n",
    "    if is_mpijob:\n",
    "        print(f\"Rank #{rank} result: {result}\")\n",
    "        # Collect the result from all ranks to the root rank (#0):\n",
    "        result = comm.reduce(result, op=MPI.SUM, root=0)\n",
    "        \n",
    "    # Log the values (only from root rank (#0) in mpijob):\n",
    "    if rank == 0:\n",
    "        array = np.arange(100 + int(result) % 1000)\n",
    "        run_time = time.time() - run_time\n",
    "        return run_time, result, array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0053bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbd3ca",
   "metadata": {},
   "source": [
    "## 3. Create a Project\n",
    "\n",
    "1. Create the MLRun project.\n",
    "2. Create an MLRun function of the processing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a97e79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the project:\n",
    "project = mlrun.get_or_create_project(name=S3_PROJECT_DIRECTORY, context=\"./\", user_project=False)\n",
    "\n",
    "# Add the S3 credentials:\n",
    "project.set_secrets(\n",
    "    secrets={\n",
    "        \"AWS_ACCESS_KEY_ID\": AWS_ACCESS_KEY_ID,\n",
    "        \"AWS_SECRET_ACCESS_KEY\": AWS_SECRET_ACCESS_KEY,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ffec4-9cb8-4862-b678-229b3f988016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the job function:\n",
    "job_function = project.set_function(name=\"process_data_job\", kind=\"job\", image=\"mlrun/mlrun\", handler=\"process_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60b73b-f271-4135-865e-937f629a922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the open mpi function:\n",
    "mpijob_function = project.set_function(name=\"process_data_mpijob\", kind=\"mpijob\", image=\"mlrun/mlrun\", handler=\"process_data\")\n",
    "mpijob_function.spec.replicas = N_RANKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b077007",
   "metadata": {},
   "source": [
    "## 4. Run As A Job\n",
    "\n",
    "Run the processing as a `job` and storing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214aaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without dask:\n",
    "job_run = job_function.run(\n",
    "    name=\"process_data_job\",\n",
    "    inputs={\n",
    "        \"data_path\": f\"s3://{S3_BUCKET}/{S3_PROJECT_DIRECTORY}/{DATA_PATH}/\",\n",
    "    },\n",
    "    artifact_path=f\"s3://{S3_BUCKET}/{S3_PROJECT_DIRECTORY}\",\n",
    "    returns=[\"time\", \"result\", \"array:dataset\"],\n",
    ")\n",
    "\n",
    "# Store results:\n",
    "job_time = job_run.status.results['time']\n",
    "job_result = job_run.status.results['result']\n",
    "job_array = np.array(job_run.artifact('array').as_df()[\"0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a495acb",
   "metadata": {},
   "source": [
    "## 5. Run As a MPIJob\n",
    "\n",
    "Run the processing as a `mpijob` and storing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run without dask:\n",
    "mpijob_run = mpijob_function.run(\n",
    "    name=\"process_data_mpijob\",\n",
    "    inputs={\n",
    "        \"data_path\": f\"s3://{S3_BUCKET}/{S3_PROJECT_DIRECTORY}/{DATA_PATH}/\",\n",
    "    },\n",
    "    artifact_path=f\"s3://{S3_BUCKET}/{S3_PROJECT_DIRECTORY}\",\n",
    "    returns=[\"time\", \"result\", \"array:dataset\"],\n",
    ")\n",
    "\n",
    "# Store results:\n",
    "mpijob_time = mpijob_run.status.results['time']\n",
    "mpijob_result = mpijob_run.status.results['result']\n",
    "mpijob_array = np.array(mpijob_run.artifact('array').as_df()[\"0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24830791",
   "metadata": {},
   "source": [
    "## 6. Compare Runtimes\n",
    "\n",
    "1. Print a summary message.\n",
    "2. Verify that:\n",
    "  * The mpijob run took less time (only in stronger machines). \n",
    "  * The result value is equal between the runs.\n",
    "  * The array values are equal between the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the project and data in S3:\n",
    "s3_client.delete(\n",
    "    bucket=S3_BUCKET,\n",
    "    s3_path=S3_PROJECT_DIRECTORY,\n",
    ")\n",
    "\n",
    "# Delete the MLRun project:\n",
    "mlrun.get_run_db().delete_project(name=project.name, deletion_strategy=\"cascading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f164230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the test's collected results:\n",
    "print(\n",
    "    f\"Job:\\n\" \n",
    "    f\"\\t{'%.2f' % job_time} Seconds\\n\"\n",
    "    f\"\\tResult: {job_result}\"\n",
    ")\n",
    "print(\n",
    "    f\"Open MPI Job:\\n\"\n",
    "    f\"\\t{'%.2f' % mpijob_time} Seconds\\n\"\n",
    "    f\"\\tResult: {mpijob_result}\\n\"\n",
    ")\n",
    "\n",
    "# Verification:\n",
    "# assert mpijob_time < job_time  # Only possible to test on a stronger machine as the test requires big data.\n",
    "assert np.isclose(job_result, mpijob_result)\n",
    "assert all(job_array == mpijob_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
